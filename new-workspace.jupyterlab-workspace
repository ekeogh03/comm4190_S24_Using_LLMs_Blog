{"data":{"layout-restorer:data":{"main":{"dock":{"type":"tab-area","currentIndex":0,"widgets":["editor:comm4190_S24/healthcomm/README.md","notebook:comm4190_S24/healthcomm/FinalCampaignImages.ipynb"]},"current":"editor:comm4190_S24/healthcomm/README.md"},"down":{"size":0,"widgets":[]},"left":{"collapsed":false,"visible":true,"current":"filebrowser","widgets":["filebrowser","running-sessions","git-sessions","@jupyterlab/toc:plugin","jupyterlab-citation-manager:reference-browser"]},"right":{"collapsed":true,"visible":true,"widgets":["jp-property-inspector","debugger-sidebar"]},"relativeSizes":[0.17890235210263722,0.8210976478973628,0],"top":{"simpleVisibility":true}},"file-browser-filebrowser:cwd":{"path":"comm4190_S24/healthcomm"},"workspace-ui:lastSave":"comm4190_S24/comm4190_S24_Using_LLMs_Blog/new-workspace.jupyterlab-workspace","editor:comm4190_S24/healthcomm/README.md":{"data":{"path":"comm4190_S24/healthcomm/README.md","factory":"Editor"}},"jupyterlab-citation-manager:zotero":{"persistentCacheVersion":"0..","apiVersion":"3","lastModifiedLibraryVersion":"43","citableItems":{"15913771/AGK7YRIW":{"id":"15913771/AGK7YRIW","type":"article","title":"ScienceDirect Snapshot"},"15913771/IPFXQ76D":{"id":"15913771/IPFXQ76D","type":"article","title":"Full Text PDF"},"15913771/IMF96AP9":{"id":"15913771/IMF96AP9","type":"article","title":"Full Text PDF"},"15913771/U6HEE4JT":{"id":"15913771/U6HEE4JT","type":"article-journal","title":"The effect of gender stereotypes on artificial intelligence recommendations","container-title":"Journal of Business Research","page":"50–59","volume":"141","abstract":"This journal article discusses how AI perceieves male and female interactions. For example, there was more warmth with a female AI agent was involved compared to the male ai agent. It goes on to discuss the persuasion technqiues that gender influenced ai can use. For example, selling utilitarian products is more successful by a male ai agent, but more hedonic products are sold more succesfully through female ai agents. This research can be used on how gender stereotyped ai can infleunce human decisions.","URL":"https://www.sciencedirect.com/science/article/pii/S0148296321009206","DOI":"10.1016/j.jbusres.2021.12.007","author":[{"family":"Ahn","given":"Jungyong"},{"family":"Kim","given":"Jungwon"},{"family":"Sung","given":"Yongjun"}],"issued":{"date-parts":[[2022,3]]},"accessed":{"date-parts":[[2023,10,11]]}},"15913771/I25IZ8B7":{"id":"15913771/I25IZ8B7","type":"article-journal","title":"A systematic review of socio-technical gender bias in AI algorithms","container-title":"Online Information Review","volume":"ahead-of-print","issue":"ahead-of-print","abstract":"This is a study about the socio-technical impacts on gender biases with AI. The journal advocates for less of a focus on the technology and more of a deepdive into the development, design, and critique of it. They argue that gender is not binary and leading with too simple of an algorithm will bound to lead to inconsistencies. Along with other articles, this one believes more women involved with the development and management will help decrease bias. It also has some good graphs on the data they found surrounding results for social solutions, social solutions related to algorithmic design, and social consequence.","URL":"https://doi.org/10.1108/OIR-08-2021-0452","DOI":"10.1108/OIR-08-2021-0452","author":[{"family":"Hall","given":"Paula"},{"family":"Ellis","given":"Debbie"}],"issued":{"date-parts":[[2023,1]]},"accessed":{"date-parts":[[2023,10,9]]}},"15913771/7CW8JZ9W":{"id":"15913771/7CW8JZ9W","type":"article","title":"Snapshot"},"15913771/R9C2LSS3":{"id":"15913771/R9C2LSS3","type":"article-journal","title":"Dealing with Gender Bias Issues in Data-Algorithmic Processes: A Social-Statistical Perspective","container-title":"Algorithms","page":"303","volume":"15","issue":"9","abstract":"This journal article walks us through the definiton of gender bias in AI, the spike in popularity amid the topic, how an algorithm works, and specific examples. Almost all of the problems regarding social and gender biases in AI stem from the lack of diverse data in the datasets that the AI pulls from. This article also addresses initiatives for gender biases like international organizations and privatized inititives, along with recommendations on how to decrease these biases.","URL":"https://www.mdpi.com/1999-4893/15/9/303","DOI":"10.3390/a15090303","shortTitle":"Dealing with Gender Bias Issues in Data-Algorithmic Processes","language":"en","author":[{"family":"Castaneda","given":"Juliana"},{"family":"Jover","given":"Assumpta"},{"family":"Calvet","given":"Laura"},{"family":"Yanes","given":"Sergi"},{"family":"Juan","given":"Angel A."},{"family":"Sainz","given":"Milagros"}],"issued":{"date-parts":[[2022,9]]},"accessed":{"date-parts":[[2023,10,9]]}},"15913771/T47BIMWG":{"id":"15913771/T47BIMWG","type":"article","title":"AI Bias Could Put Women’s Lives At Risk - A Challenge For Regulators"},"15913771/BGGF9QCJ":{"id":"15913771/BGGF9QCJ","type":"article","title":"Snapshot"},"15913771/FD3S9T3W":{"id":"15913771/FD3S9T3W","type":"article","title":"How can we solve the problems of gender bias in AI? Experts weigh in.","abstract":"This blog post brings up a cool idea about a hippocratic oath for AI, meaning that there should be a legal and ethical document promising to do no hard with the technology surrounding AI. If an oath is too hard to sustain, then a checklist needs to be used to be able to operate this technology keeping ethics, biases, and safety in mind. The blog agrees that AI biases are not at fault from the technology but rather the humans that built it. There solution is for women to be included at every stage in the development and distribution process of AI.","URL":"https://aiforgood.itu.int/how-can-we-solve-the-problems-of-gender-bias-in-ai-experts-weigh-in/","note":"Publication Title: AI for Good","shortTitle":"How can we solve the problems of gender bias in AI?","language":"en-US","author":[{"family":"aiforgoodstg2","given":""}],"issued":{"date-parts":[[2020,5]]},"accessed":{"date-parts":[[2023,10,9]]}},"15913771/ZI3P7EC5":{"id":"15913771/ZI3P7EC5","type":"article","title":"Addressing Gender Bias to Achieve Ethical AI","abstract":"This source highlights that only 12% of AI researchers are women and 6% of software researchers are women. This gap in the workfield manifests itself into major biases regarding AI. An example this article dives into is about Amazon's hiring algorithm. It had AI take applications from the last 10 years and analyze the top qualifications from the applications accepted to decide the best canidates from a recent batch. The algorithm was skewed though as the sample from which AI was collecting from was male dominated and did not reflect the accurate number of women in the workplace. The article includes more examples (this source will have lots of info).","URL":"https://theglobalobservatory.org/2023/03/gender-bias-ethical-artificial-intelligence/","note":"Publication Title: IPI Global Observatory","language":"en-US","author":[{"family":"Manasi","given":"Ardra"},{"family":"Panchanadeswaran","given":"Subadra"},{"family":"Sours","given":"Emily"}],"issued":{"date-parts":[[2023,3]]},"accessed":{"date-parts":[[2023,10,9]]}},"15913771/GF8F45EY":{"id":"15913771/GF8F45EY","type":"article","title":"Snapshot"},"15913771/AYEMUU52":{"id":"15913771/AYEMUU52","type":"article","title":"Snapshot"},"15913771/IIW5WEQ2":{"id":"15913771/IIW5WEQ2","type":"article","title":"AI Bias Could Put Women’s Lives At Risk - A Challenge For Regulators","abstract":"This sources highlights examples of risk of using male dominated date to supply the patterns of code for AI. They bring up an example how seatbelts were created with male dummies to test on, but pregnant women dummies for example are rarely used which creates a safety bias. Applying this thinking for AI, if the majority of our data sources are supplied and interpretted by men then there is an innate bias that will be hard to overcome. This leads to major risks for women, especially in the health data world. Health monitoring apps already show bias if men and women input similar symptoms men will be more likely to be shown they are having a heart attack, while a women's results will say it's depression. This article also suggests more women in tech to help combat amplified biases.","URL":"https://www.forbes.com/sites/carmenniethammer/2020/03/02/ai-bias-could-put-womens-lives-at-riska-challenge-for-regulators/?sh=4a288e9c534f","accessed":{"date-parts":[[2023,10,9]]}},"15913771/L5BMC3EJ":{"id":"15913771/L5BMC3EJ","type":"article","title":"Gender stereotyping","abstract":"This paper is a definition source of sorts for me. It goes into detail about what it means to gender stereotype and how gender biases are formed from human behavior not from an AI standpoint. It's important to understand it without the new technology before jumping into the complication revolving around AI. This source brings in international laws that oppose harmful stereotypes, like elimination of discrimination against women and combatting stereotypes of people with disabilities, which I will use to argue the destructive implications of gender biases from AI.","URL":"https://www.ohchr.org/en/women/gender-stereotyping","note":"Publication Title: OHCHR","language":"en","accessed":{"date-parts":[[2023,10,9]]}},"15913771/XHD2MH4I":{"id":"15913771/XHD2MH4I","type":"article","title":"Gender and AI: Addressing bias in artificial intelligence","abstract":"As pointed out in tihis article, \"Ai is a mirror of ourselves\" which implies that AI has all the same biases that us humans do. An example pulled from Harvard Business Review cites that Siri, Alexa, and other AI machines play word associations like doctor with man and nurse with woman, but these are outdated and wrong assuming. The article thinks that having more women in the tech field will lead to less biases. Awesome quote: “In this bleak depiction of our future, decades of fights for civil rights and equality have been unwritten in a few lines of code.”","URL":"https://www.internationalwomensday.com/Missions/14458/Gender-and-AI-Addressing-bias-in-artificial-intelligence","note":"Publication Title: International Women's Day","shortTitle":"Gender and AI","language":"en","author":[{"family":"Day","given":"International Women's"}],"accessed":{"date-parts":[[2023,10,9]]}},"15913771/GCJE9JYX":{"id":"15913771/GCJE9JYX","type":"article","title":"Snapshot","URL":"https://www.technologyreview.com/2017/10/24/148218/the-dangers-of-tech-bro-ai/","accessed":{"date-parts":[[2023,10,23]]}},"15913771/Z98XB9FV":{"id":"15913771/Z98XB9FV","type":"webpage","title":"The Dangers of Tech-Bro AI","container-title":"MIT Technology Review","abstract":"Tabitha Goldstaub, a cofounder of CognitionX, which \nhelps companies deploy AI, says that diversifying the field \nis necessary to make sure products actually work well.","URL":"https://www.technologyreview.com/2017/10/24/148218/the-dangers-of-tech-bro-ai/","language":"en","accessed":{"date-parts":[[2023,10,23]]}},"15913771/VJ7V5SQY":{"id":"15913771/VJ7V5SQY","type":"article","title":"Why don’t European girls like science or technology? - Microsoft News Centre Europe","URL":"https://news.microsoft.com/europe/features/dont-european-girls-like-science-technology/#sm.0000a046evm91crtzzd15dbmak88g%23O0g4dh2732ZlhJdB.97","accessed":{"date-parts":[[2023,10,22]]}},"15913771/8WAIKCEM":{"id":"15913771/8WAIKCEM","type":"webpage","title":"Why don’t European girls like science or technology? - Microsoft News Centre Europe","URL":"https://news.microsoft.com/europe/features/dont-european-girls-like-science-technology/#sm.0000a046evm91crtzzd15dbmak88g%23O0g4dh2732ZlhJdB.97","accessed":{"date-parts":[[2023,10,22]]}},"15913771/TBM3FAKB":{"id":"15913771/TBM3FAKB","type":"webpage","title":"This is the Exact Age When Girls Lose Interest in Math and Science - Tinybeans","URL":"https://tinybeans.com/this-is-the-exact-age-when-girls-lose-interest-in-math-and-science/","accessed":{"date-parts":[[2023,10,22]]}},"15913771/ANZVEBK3":{"id":"15913771/ANZVEBK3","type":"article","title":"Snapshot","URL":"https://hai.stanford.edu/news/how-has-covid-affected-ai-economy#","accessed":{"date-parts":[[2023,10,22]]}},"15913771/F779I3G9":{"id":"15913771/F779I3G9","type":"webpage","title":"How Has COVID Affected the AI Economy?","container-title":"Stanford HAI","abstract":"The AI Index finds that despite the pandemic’s economic hit, AI investment and hiring increased.","URL":"https://hai.stanford.edu/news/how-has-covid-affected-ai-economy","language":"en","accessed":{"date-parts":[[2023,10,22]]}},"15913771/988VZBTK":{"id":"15913771/988VZBTK","type":"article-journal","title":"The effect of gender stereotypes on artificial intelligence recommendations","container-title":"Journal of Business Research","page":"50-59","volume":"141","abstract":"This journal article discusses how AI perceieves male and female interactions. For example, there was more warmth with a female AI agent was involved compared to the male ai agent. It goes on to discuss the persuasion technqiues that gender influenced ai can use. For example, selling utilitarian products is more successful by a male ai agent, but more hedonic products are sold more succesfully through female ai agents. This research can be used on how gender stereotyped ai can infleunce human decisions.","URL":"https://www.sciencedirect.com/science/article/pii/S0148296321009206","DOI":"10.1016/j.jbusres.2021.12.007","journalAbbreviation":"Journal of Business Research","author":[{"family":"Ahn","given":"Jungyong"},{"family":"Kim","given":"Jungwon"},{"family":"Sung","given":"Yongjun"}],"issued":{"date-parts":[[2022,3,1]]},"accessed":{"date-parts":[[2023,10,11]]}},"15913771/YKUBBF96":{"id":"15913771/YKUBBF96","type":"article","title":"ScienceDirect Snapshot","URL":"https://www.sciencedirect.com/science/article/pii/S0148296321009206","accessed":{"date-parts":[[2023,10,11]]}},"15913771/4TUN8TUP":{"id":"15913771/4TUN8TUP","type":"article-journal","title":"A systematic review of socio-technical gender bias in AI algorithms","container-title":"Online Information Review","volume":"ahead-of-print","issue":"ahead-of-print","abstract":"This is a study about the socio-technical impacts on gender biases with AI. The journal advocates for less of a focus on the technology and more of a deepdive into the development, design, and critique of it.  They argue that gender is not binary and leading with too simple of an algorithm will bound to lead to inconsistencies. Along with other articles, this one believes more women involved with the development and management will help decrease bias. It also has some good graphs on the data they found surrounding results for social solutions, social solutions related to algorithmic design, and social consequence.","URL":"https://doi.org/10.1108/OIR-08-2021-0452","DOI":"10.1108/OIR-08-2021-0452","author":[{"family":"Hall","given":"Paula"},{"family":"Ellis","given":"Debbie"}],"issued":{"date-parts":[[2023,1,1]]},"accessed":{"date-parts":[[2023,10,9]]}},"15913771/L546C7BW":{"id":"15913771/L546C7BW","type":"article","title":"Full Text PDF","URL":"https://www.emerald.com/insight/content/doi/10.1108/OIR-08-2021-0452/full/pdf?title=a-systematic-review-of-socio-technical-gender-bias-in-ai-algorithms","accessed":{"date-parts":[[2023,10,9]]}},"15913771/4M2V6UPB":{"id":"15913771/4M2V6UPB","type":"article-journal","title":"Dealing with Gender Bias Issues in Data-Algorithmic Processes: A Social-Statistical Perspective","container-title":"Algorithms","page":"303","volume":"15","issue":"9","abstract":"This journal article walks us through the definiton of gender bias in AI, the spike in popularity amid the topic, how an algorithm works, and specific examples. Almost all of the problems regarding social and gender biases in AI stem from the lack of diverse data in the datasets that the AI pulls from. This article also addresses initiatives for gender biases like international organizations and privatized inititives, along with recommendations on how to decrease these biases.","URL":"https://www.mdpi.com/1999-4893/15/9/303","DOI":"10.3390/a15090303","note":"Number: 9\nPublisher: Multidisciplinary Digital Publishing Institute","shortTitle":"Dealing with Gender Bias Issues in Data-Algorithmic Processes","language":"en","author":[{"family":"Castaneda","given":"Juliana"},{"family":"Jover","given":"Assumpta"},{"family":"Calvet","given":"Laura"},{"family":"Yanes","given":"Sergi"},{"family":"Juan","given":"Angel A."},{"family":"Sainz","given":"Milagros"}],"issued":{"date-parts":[[2022,9]]},"accessed":{"date-parts":[[2023,10,9]]}},"15913771/MXMSG9CB":{"id":"15913771/MXMSG9CB","type":"article","title":"Full Text PDF","URL":"https://www.mdpi.com/1999-4893/15/9/303/pdf?version=1663756012","accessed":{"date-parts":[[2023,10,9]]}},"15913771/UP78RV6M":{"id":"15913771/UP78RV6M","type":"post-weblog","title":"Addressing Gender Bias to Achieve Ethical AI","container-title":"IPI Global Observatory","abstract":"This source highlights that only 12% of AI researchers are women and 6% of software researchers are women. This gap in the workfield manifests itself into major biases regarding AI. An example this article dives into is about Amazon's hiring algorithm. It had AI take applications from the last 10 years and analyze the top qualifications from the applications accepted to decide the best canidates from a recent batch. The algorithm was skewed though as the sample from which AI was collecting from was male dominated and did not reflect the accurate number of women in the workplace. The article includes more examples (this source will have lots of info).","URL":"https://theglobalobservatory.org/2023/03/gender-bias-ethical-artificial-intelligence/","note":"Section: Analysis","language":"en-US","author":[{"family":"Manasi","given":"Ardra"},{"family":"Panchanadeswaran","given":"Subadra"},{"family":"Sours","given":"Emily"}],"issued":{"date-parts":[[2023,3,17]]},"accessed":{"date-parts":[[2023,10,9]]}},"15913771/EW4F4ZUE":{"id":"15913771/EW4F4ZUE","type":"webpage","title":"AI Bias Could Put Women’s Lives At Risk - A Challenge For Regulators","abstract":"This sources highlights examples of risk of using male dominated date to supply the patterns of code for AI. They bring up an example how seatbelts were created with male dummies to test on, but pregnant women dummies for example are rarely used which creates a safety bias. Applying this thinking for AI, if the majority of our data sources are supplied and interpretted by men then there is an innate bias that will be hard to overcome. This leads to major risks for women, especially in the health data world. Health monitoring apps already show bias if men and women input similar symptoms men will be more likely to be shown they are having a heart attack, while a women's results will say it's depression. This article also suggests more women in tech to help combat amplified biases.","URL":"https://www.forbes.com/sites/carmenniethammer/2020/03/02/ai-bias-could-put-womens-lives-at-riska-challenge-for-regulators/?sh=4a288e9c534f","accessed":{"date-parts":[[2023,10,9]]}},"15913771/UILJSGYY":{"id":"15913771/UILJSGYY","type":"article","title":"Snapshot","URL":"https://theglobalobservatory.org/2023/03/gender-bias-ethical-artificial-intelligence/#","accessed":{"date-parts":[[2023,10,9]]}},"15913771/LMIBA2VF":{"id":"15913771/LMIBA2VF","type":"post-weblog","title":"How can we solve the problems of gender bias in AI? Experts weigh in.","container-title":"AI for Good","abstract":"This blog post brings up a cool idea about a hippocratic oath for AI, meaning that there should be a legal and ethical document promising to do no hard with the technology surrounding AI. If an oath is too hard to sustain, then a checklist needs to be used to be able to operate this technology keeping ethics, biases, and safety in mind. The blog agrees that AI biases are not at fault from the technology but rather the humans that built it. There solution is for women to be included at every stage in the development and distribution process of AI.","URL":"https://aiforgood.itu.int/how-can-we-solve-the-problems-of-gender-bias-in-ai-experts-weigh-in/","shortTitle":"How can we solve the problems of gender bias in AI?","language":"en-US","author":[{"family":"aiforgoodstg2","given":""}],"issued":{"date-parts":[[2020,5,6]]},"accessed":{"date-parts":[[2023,10,9]]}},"15913771/CQBA2VZB":{"id":"15913771/CQBA2VZB","type":"article","title":"Snapshot","URL":"https://aiforgood.itu.int/how-can-we-solve-the-problems-of-gender-bias-in-ai-experts-weigh-in/","accessed":{"date-parts":[[2023,10,9]]}},"15913771/V95UV2KW":{"id":"15913771/V95UV2KW","type":"article","title":"AI Bias Could Put Women’s Lives At Risk - A Challenge For Regulators","URL":"https://www.forbes.com/sites/carmenniethammer/2020/03/02/ai-bias-could-put-womens-lives-at-riska-challenge-for-regulators/?sh=4a288e9c534f","accessed":{"date-parts":[[2023,10,9]]}},"15913771/ZETRE28D":{"id":"15913771/ZETRE28D","type":"webpage","title":"Gender and AI: Addressing bias in artificial intelligence","container-title":"International Women's Day","abstract":"As pointed out in tihis article, \"Ai is a mirror of ourselves\" which implies that AI has all the same biases that us humans do. An example pulled from Harvard Business Review cites that Siri, Alexa, and other AI machines play word associations like doctor with man and nurse with woman, but these are outdated and wrong assuming. The article thinks that having more women in the tech field will lead to less biases. Awesome quote: “In this bleak depiction of our future, decades of fights for civil rights and equality have been unwritten in a few lines of code.”","URL":"https://www.internationalwomensday.com/Missions/14458/Gender-and-AI-Addressing-bias-in-artificial-intelligence","shortTitle":"Gender and AI","language":"en","author":[{"family":"Day","given":"International Women's"}],"accessed":{"date-parts":[[2023,10,9]]}},"15913771/IZZPGT49":{"id":"15913771/IZZPGT49","type":"article","title":"Snapshot","URL":"https://www.internationalwomensday.com/Missions/14458/Gender-and-AI-Addressing-bias-in-artificial-intelligence","accessed":{"date-parts":[[2023,10,9]]}},"15913771/T6K97MLQ":{"id":"15913771/T6K97MLQ","type":"webpage","title":"Gender stereotyping","container-title":"OHCHR","abstract":"This paper is a definition source of sorts for me. It goes into detail about what it means to gender stereotype and how gender biases are formed from human behavior not from an AI standpoint. It's important to understand it without the new technology before jumping into the complication revolving around AI. This source brings in international laws that oppose harmful stereotypes, like elimination of discrimination against women and combatting stereotypes of people with disabilities, which I will use to argue the destructive implications of gender biases from AI.","URL":"https://www.ohchr.org/en/women/gender-stereotyping","language":"en","accessed":{"date-parts":[[2023,10,9]]}},"15913771/7U9UIUYA":{"id":"15913771/7U9UIUYA","type":"article","title":"Snapshot","URL":"https://www.ohchr.org/en/women/gender-stereotyping","accessed":{"date-parts":[[2023,10,9]]}}}},"notebook:comm4190_S24/healthcomm/FinalCampaignImages.ipynb":{"data":{"path":"comm4190_S24/healthcomm/FinalCampaignImages.ipynb","factory":"Notebook"}}},"metadata":{"id":"new-workspace","last_modified":"2024-03-18T19:23:06.565158+00:00","created":"2024-03-18T19:23:06.565158+00:00"}}